{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#svm and navie bayes assignment\n"
      ],
      "metadata": {
        "id": "6KbX20S9KN2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#question\n"
      ],
      "metadata": {
        "id": "abfIyxWXKVEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "2.Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "3.What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "4.What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "5.Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n",
        "\n",
        " 6.Write a Python program to: ● Load the Iris dataset ● Train an SVM Classifier with a linear kernel ● Print the model's accuracy and support vectors.\n",
        "\n",
        "7.Write a Python program to: ● Load the Breast Cancer dataset ● Train a Gaussian Naïve Bayes model ● Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "8.Write a Python program to: ● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma. ● Print the best hyperparameters and accuracy.\n",
        "\n",
        "9.Write a Python program to: ● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups). ● Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "10.Imagine you’re working as a data scientist for a company that handles email communications. Your task is to automatically classify emails as Spam or Not Spam. The emails may contain: ● Text with diverse vocabulary ● Potential class imbalance (far more legitimate emails than spam) ● Some incomplete or missing data Explain the approach you would take to: ● Preprocess the data (e.g. text vectorization, handling missing data) ● Choose and justify an appropriate model (SVM vs. Naïve Bayes) ● Address class imbalance ● Evaluate the performance of your solution with suitable metrics And explain the business impact of your solution.\n"
      ],
      "metadata": {
        "id": "H3cr_iyKKYsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#answers\n"
      ],
      "metadata": {
        "id": "JkXdohDlLXr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. SVM:\n",
        "A supervised ML algorithm that finds the best hyperplane separating classes with the maximum margin.\n",
        "\n",
        "2. Hard vs. Soft Margin:\n",
        "\n",
        "Hard Margin: No misclassifications allowed (perfect separation, used for linearly separable data).\n",
        "\n",
        "Soft Margin: Allows some misclassifications using a penalty parameter (C) for better generalization.\n",
        "\n",
        "3. Kernel Trick:\n",
        "Transforms data into higher dimensions to make it linearly separable.\n",
        "Example: RBF kernel — useful for non-linear data like circular patterns.\n",
        "\n",
        "4. Naïve Bayes:\n",
        "A probabilistic classifier using Bayes’ theorem assuming feature independence — hence “naïve.”\n",
        "\n",
        "5. Types:\n",
        "\n",
        "Gaussian: Continuous data (e.g., height, weight).\n",
        "\n",
        "Multinomial: Discrete counts (e.g., word frequencies).\n",
        "\n",
        "Bernoulli: Binary features (e.g., word presence/absence)."
      ],
      "metadata": {
        "id": "b5wfPGMML5JZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNy9aS1eKBCT",
        "outputId": "68f9f266-3e9f-47a4-a7e5-4f8c2b1d05bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9933333333333333\n",
            "Support Vectors: [[5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [5.9 3.  5.1 1.8]]\n"
          ]
        }
      ],
      "source": [
        "#ans6\n",
        "from sklearn import datasets, svm, metrics\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "model = svm.SVC(kernel='linear')\n",
        "model.fit(X, y)\n",
        "print(\"Accuracy:\", model.score(X, y))\n",
        "print(\"Support Vectors:\", model.support_vectors_)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ans7\n",
        "from sklearn import datasets, metrics\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
        "model = GaussianNB().fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "print(metrics.classification_report(y, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQIyhIK-MYN7",
        "outputId": "3dc46fa7-99fa-47fc-b364-b75d9c103c33"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.89      0.92       212\n",
            "           1       0.94      0.97      0.95       357\n",
            "\n",
            "    accuracy                           0.94       569\n",
            "   macro avg       0.94      0.93      0.94       569\n",
            "weighted avg       0.94      0.94      0.94       569\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ans8\n",
        "from sklearn import datasets, svm, model_selection, metrics\n",
        "X, y = datasets.load_wine(return_X_y=True)\n",
        "params = {'C':[0.1,1,10], 'gamma':[0.001,0.01,0.1]}\n",
        "grid = model_selection.GridSearchCV(svm.SVC(), params, cv=5)\n",
        "grid.fit(X, y)\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au8BfqHRMfSW",
        "outputId": "b8c02ce1-46e3-47bf-f43c-46fdd3d4c77f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'C': 10, 'gamma': 0.001}\n",
            "Accuracy: 0.7533333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ans9\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "data = fetch_20newsgroups(subset='train')\n",
        "X = TfidfVectorizer().fit_transform(data.data)\n",
        "y = data.target\n",
        "model = MultinomialNB().fit(X, y)\n",
        "print(\"ROC-AUC:\", roc_auc_score(y, model.predict_proba(X), multi_class='ovr'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-UXAwO1MhDm",
        "outputId": "bfe35d92-91a3-4649-b425-396a82abf51f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC: 0.9981134090459266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ans10\n",
        "Spam Detection Approach (Short):\n",
        "\n",
        "Preprocessing: Clean text, fill missing data, apply TF-IDF vectorization.\n",
        "\n",
        "Model Choice: Naïve Bayes (fast, effective for text & word frequencies).\n",
        "\n",
        "Class Imbalance: Use SMOTE or class weights.\n",
        "\n",
        "Evaluation: Precision, recall, F1-score, ROC-AUC.\n",
        "\n",
        "Business Impact: Reduces spam, saves user time, boosts productivity and trust."
      ],
      "metadata": {
        "id": "oPB6BVX9MjKA"
      }
    }
  ]
}